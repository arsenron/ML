{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# &emsp;    Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;This work is focused  mainly on deriving backpropagation formulas and explaining the implementation of backpropagation so forward propagation is discussed briefly. There are a lot of good articles on forward propagation and it is very easy concept that can be completely understood in a maximum of a few hours. But we want to build a complete model, so forward propagation is the first part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Let's begin with the notation. I don't want to reinvent the wheel, so the notation will be similar to common notation in most of other texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &emsp;&emsp; Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;We are going to have $L$ layers, columns represent examples, for ex, 20 columns represent 20 examples.  <br>\n",
    " - $n^l$ - number of units in layer $l$ <br>\n",
    " - $n^{l - 1}$ - number of units in layer $l - 1$ <br>\n",
    " - $a^0$ = $x$ - input vector from $R^n$ <br>\n",
    " - $A^0$ = $X$ - input matrix from $R^{nxm}$ <br>\n",
    " - $a^l$ - activation vector from $R^{{n^l}x1}$ <br>\n",
    " - $A^l$ - activation matrix from $R^{{n^l}xm}$ <br>\n",
    " - $z^l$ - weighted input vector from $R^{{n^l}x1}$<br>\n",
    " - $Z^l$ - weighted input matrix from $R^{{n^l}xm}$<br>\n",
    " - $g(x)$ - activation function <br>\n",
    " - $A^L$ - activation from the last layer $L$, that is the final output <br>\n",
    " - $W^l$ - weight matrix to the $l$-th layer from $R^{n^lxn^{l-1}}$. A j-th row of a matrix represent coefficients, that relate to the j-th unit in the l-th layer, they \"weight\" activation from the previous layer. <br>\n",
    " - $b^l$ - bias to $l$-th layer from $R^{{n^l}x1}$ <br>\n",
    " - $C(a^l)$ - cost function of one example <br>\n",
    " - $J(A^l)$ = $\\frac{\\sum_{i=1}^{m}C(a^l)}{m}$  - cost function of m examples - just applying $C(a^l)$ to all of the examples and then finding it's mean. <br>\n",
    "<br>\n",
    "<br>\n",
    "When we use subscripts, it means the unit number. For instance, $a_k^l$, means the activation \n",
    "of $k$-th unit in the $l$-th layer.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Let's build forward propagation and in backpropagation part we will a little bit extend the notation to get formulas in convinient form. After all the functions are written, we will make a class in Python to combine all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Formulas for forward propagation are the following: <br>\n",
    "&emsp;&emsp;For one example: <br><br>\n",
    "&emsp;&emsp;&emsp;&emsp;1) $z^l = W^l a^{l - 1} + b^l$ <br>\n",
    "&emsp;&emsp;&emsp;&emsp;2) $z^l = g(z^l)$ <br><br>\n",
    "&emsp;&emsp;For m examples (matrix form): <br><br>\n",
    "&emsp;&emsp;&emsp;&emsp;3) $Z^l = W^l A^{l - 1} + b^l$ <br>\n",
    "&emsp;&emsp;&emsp;&emsp;4) $A^l = g(Z^l)$ <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Important note: We consider $b^l$ usually as a vector, but when we are working with m examples simultaneously, it is obvious that $b^l$ would be a matrix, so it equals to vector of ones from $R^{mx1}$ multiplied by $(b^l)^T$ (our bias vector transposed) or, in other words, just extended first column, so the first column equals to the second and so on to the m-th column. But thanks to Python, we don't need to explicitly always mention about it, because in Python's numpy library the operation we have discussed is done automatically $-$ when we add matrix $W^l A^{l - 1}$ in our case to vector $b^l$. So we have no urgent need to extend our notation to $b^l$ in matrix form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp; As our notebook is for explaining backpropagation, forward propagation won't be touched from now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Let's extend notation a little bit. We call \"error\" of layer l the following: <br>\n",
    " - $\\delta^l$ $\\equiv$ $\\frac{\\partial C}{\\partial z^l} \\in R^{{n^l}x1}$  <br>\n",
    " - $\\Delta^l$ $\\equiv$ $\\frac{\\partial J}{\\partial Z^l} \\in R^{{n^l}xm}$ <br><br>\n",
    "&emsp;&emsp;We can think of error just as how our cost function is changed, when we add a very little quantity to each element of weighted input from the $l$-th layer separately (In the beginning to the first element of weighted input, then compute cost, derivative and so on), and compute the difference between obtained value of cost function and initial value and divide it by that very little quantity, we added to an input in layer $l$ (It is just the authentic meaning of derivative concept). \n",
    "Note that the second formula is the same as the first, but just for m examples. And also \"errors\" have the same dimension, as $z^l$ for one example (observation) or as $Z^l$ for m examples. We introduced error as it will help us a lot for reaching the final purpose - to compute gradients of cost function w.r.t. $W^l$ and $b^l$. It is because we stand one step away from computing those quantaties. Due to chain rule, we obtain the following (see picture below): <br><br>\n",
    "1) $\\frac{\\partial C}{\\partial W^l} = \\frac{\\partial C}{\\partial z^l}\\frac{\\partial z^l}{\\partial W^l}$ <br><br>\n",
    "2) $\\frac{\\partial J}{\\partial W^l} = \\frac{\\partial J}{\\partial Z^l}\\frac{\\partial Z^l}{\\partial W^l}$ <br><br>\n",
    "3) $\\frac{\\partial C}{\\partial b^l} = \\frac{\\partial C}{\\partial z^l}\\frac{\\partial z^l}{\\partial b^l}$ <br><br>\n",
    "4) $\\frac{\\partial J}{\\partial b^l} = \\frac{\\partial J}{\\partial Z^l}\\frac{\\partial Z^l}{\\partial b^l}$ <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Pic1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;First, let's find $\\delta^L$ (error of the last layer, as we have $L$ layers) and then we will show that we can find recursively $\\delta^l$ from $L$-th to the first layer. It is trivial to understand that $\\delta^L = \\frac{\\partial C}{\\partial a^L}\\frac{\\partial a^L}{\\partial z^l}$, because to \"reach\" $z^L$, we first stop at $a^L$. <br>\n",
    "&emsp;&emsp;Now we are trying to find $\\delta^{l - 1}$. We know that $\\delta^{l - 1} = \\frac{\\partial C}{\\partial z^{l - 1}}$. Let's accurately, step by step, write equation for $\\delta^{l - 1}$. Let's begin with the first element of $\\delta^{l - 1} : \\delta_1^{l - 1}$(first unit error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Pic2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp; As we see in picture, first we compute $\\frac{\\partial C}{\\partial z_1^l}$, which is $\\delta_1^l$, and then we multiply it by $\\frac{\\partial z_1^l}{\\partial a_1^{l-1}}\\frac{\\partial a_1^{l-1}}{\\partial z_1^{l - 1}}$ (simple chain rule, see the picture above). But it is only the first part of our computation, because we can see that the second unit of $l$-th layer also depend on $a_1^{l - 1}$ and so $z_1^{l - 1}$. And all of the units do, because it is the essence of the algorithm, in which each unit in the current layer is a weighted output of the activation from the previous layer. So let's move a little bit further and then combine all the parts in one formula. The second part, $\\frac{\\partial z_1^l}{\\partial a_1^{l-1}}$ - it is easily observed that it will be just $w_{11}^l$. And the last part, is just derivative of activation function w.r.t. its the only argument. So in the end we obtain: $\\delta_1^{l - 1} = \\delta_1^l \\cdot w_{11}^l \\cdot \\frac{\\partial a_1^{l-1}}{\\partial z_1^{l - 1}}$. We just found the first part of the summation, but, as was mentioned, we must consider the other units that contribute to the gradient. As we see in picture, next we compute $\\delta_2^l$ and multiply it by $\\frac{\\partial z_2^l}{\\partial a_1^{l-1}}\\frac{\\partial a_1^{l-1}}{\\partial z_1^{l - 1}}$. As we see, the last part has not changed, and will never be changed, as the final operation is always the same. But the second part, $\\frac{\\partial z_2^l}{\\partial a_1^{l-1}}$ should be analyzed carefully. In our notation, first row of a weight matrix $W^l$ is related to the first unit of $z^l$ (just dot product of first row of $W^l$ and $a^{l - 1}$), second row - to the second unit. So when we take derivative of $z_2^l$ (the second unit of $l$-th layer) w.r.t. $a_1^{l-1}$, we now consider the second row of the weight matrix $W^l$, and it is obviously that it is the first element of the second row of $W^l$ : $w_{21}^l$. And if we go further, for all the units, the picture will be the same: never changed last part $\\frac{\\partial a_1^{l-1}}{\\partial z_1^{l - 1}}$ that can be factored out, the beginning part is always error : $\\delta_j^l$ (j-th unit), and the middle part is first element of j-th row of the $W^l$. So, let's summarize about the first error term: <br><br>   &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$\\delta_1^{l-1} = \\frac{\\partial a_1^{l-1}}{\\partial z_1^{l - 1}} [ (W_{j1}^l)^T(\\delta^l)]$, &nbsp;where $W_{j1}^l$ is the first column of $W^l$. <br><br>\n",
    "&emsp;&emsp; *If* we do the same steps on the second error term, we will get the following relationship in general form: <br><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "$\\delta^{l-1} = \\frac{\\partial a^{l-1}}{\\partial z^{l - 1}} \\odot ((W^l)^T\\delta^l)$, where $\\odot$ is Hadamard product or element-wise product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp; Now as we got a recursive relationship of error terms, we can think about finding out desired derivatives of cost function w.r.t. weights and biases. Let's begin with the simplest - w.r.t. biases. <br>\n",
    "&emsp;&emsp;As was mentioned before, $\\frac{\\partial C}{\\partial b^l} = \\frac{\\partial C}{\\partial z^l}\\frac{\\partial z^l}{\\partial b^l} = \\delta^l\\frac{\\partial z^l}{\\partial b^l}$. The last part, $\\frac{\\partial z^l}{\\partial b^l}$, is just vector of ones, and so due to multiplication, we can omit it. So,  we obtain: <bp>\n",
    "- $\\frac{\\partial C}{\\partial b^l} = \\delta^l$, derivative of the cost w.r.t. bias equals to the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp; Now let's derive the final part - derivative of the cost w.r.t. weights. It is clear that $\\frac{\\partial C}{\\partial W^l} = \\delta^l\\frac{\\partial z^l}{\\partial W^l}$. As took place in derivation of the recursive relationship of errors, we are also going to derive $\\frac{\\partial z^l}{\\partial W^l}$ very carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Pic3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;We can see that  $\\frac{\\partial C}{\\partial w_{11}^l} = \\frac{\\partial C}{\\partial z_1^l}\\frac{\\partial z_1^l}{\\partial w_{11}^l} = \\delta_1^l \\cdot a_1^{l-1}$ (see picture above). Let's begin with the first row and take the first coefficient from it: $w_{11}^l$. We know that coefficients of each row ($w_{11}^l, w_{12}^l, ..., w_{1n^{l-1}}^l$ - first row for instance) connected only to the $z_1^l$. So we are going to have straightforward derivation, not as previously with errors. It is clear that when we consider the first row, the first part of the formula will always be the same - $\\frac{\\partial C}{\\partial z_1^l} = \\delta_1^l$. Let's accurately look at the second part: $\\frac{\\partial z_1^l}{\\partial w_{11}^l}$. As $z_1^l$ is a linear function, it is obvious that it equals to $a_1^{l-1}$. And if we go furher, $\\frac{\\partial z_1^l}{\\partial w_{12}^l}$ and so on until the last element of the row, we will just get $a_2^{l-1}$ and to the last element - $a_{n^{l-1}}^{l-1}$. So we obtain the following formula:<br>\n",
    "- $\\frac{\\partial C}{\\partial w_{j}^l} = \\delta_j^l \\cdot a^{l-1}$, where $w_{j}^l$ - is a j-th row in a weight matrix, $\\delta_j^l$ - is a error of j-th unit. <br>\n",
    "&emsp;&emsp; Generalizing to all the rows, we have gotten the following outer product: <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial C}{\\partial W^l} = \n",
    "\\begin{pmatrix}\n",
    "\\delta_1^l\\\\\n",
    "\\delta_2^l\\\\\n",
    "...\\\\\n",
    "\\delta_{n^l}^l\\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "a_1^{l - 1}&a_2^{l - 1}&...&a_{n^{l-1}}^{l - 1}\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp; So, in matrix notation, we have obtained the following: <br>\n",
    " - $\\frac{\\partial C}{\\partial W^l} = \\delta^l (a^{l-1})^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp; Let's write all the formulas together: <br> <br>\n",
    "1)  $\\delta^L = \\frac{\\partial C}{\\partial a^L}\\frac{\\partial a^L}{\\partial z^l}$ <br> <br>\n",
    "2)  $\\delta^{l} = \\frac{\\partial a^{l}}{\\partial z^{l}} \\odot ((W^{l + 1})^T\\delta^{l+1})$ <br> <br>\n",
    "3)  $\\frac{\\partial C}{\\partial b^l} = \\delta^l$ <br> <br>\n",
    "4)  $\\frac{\\partial C}{\\partial W^l} = \\delta^l (a^{l-1})^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp; We are almost done. We are just left with generalization of these formulas to m examples, as all the formulas above have been derived for just an input vector. It is useful to check all the formulas that we are going to derive by hand, to be convinced in the correctness.  Let's begin with the first formula. It generalizes very well: <br>\n",
    " - $\\Delta^L = \\frac{\\partial J}{\\partial A^L}\\frac{\\partial A^L}{\\partial Z^l}$ - it is the same formula, vector by vector (and of course element-wise derivative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp; The generalization of second formula has the same logic: <br>\n",
    " - $\\Delta^l = \\frac{\\partial A^{l}}{\\partial Z^{l}} \\odot ((W^{l+1})^T\\Delta^{l+1})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp; But generalization of the third formula is slightly different, because we need to get vector, not a matrix, but it can be easily done, computing the mean of the columns: <br>\n",
    "- $\\frac{\\partial J}{\\partial b^l} =  \\frac{1}{m} \\sum_{i = 1}^{m}\\delta^{l(m)}$, where $\\delta^{l(m)}$ is error $\\delta^{l}$ of the m-th example, and the whole sum is just the mean of column vectors of $\\Delta^l$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp; Now we are left with the last formula, which also generalizes very well: <br>\n",
    " - $\\frac{1}{m} \\Delta^l(A^{l-1})^T$, which is a \"matrix mean\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp; Let's put all together: <br><br>\n",
    "1) $\\Delta^L = \\frac{\\partial J}{\\partial A^L}\\frac{\\partial A^L}{\\partial Z^l}$ <br><br>\n",
    "2) $\\Delta^l = \\frac{\\partial A^{l}}{\\partial Z^{l}} \\odot ((W^{l+1})^T\\Delta^{l+1})$ <br><br>\n",
    "3) $\\frac{1}{m} \\sum_{i = 1}^{m}\\delta^{l(m)}$ <br><br>\n",
    "4) $\\frac{\\partial J}{\\partial W^l} = \\frac{1}{m} \\Delta^l(A^{l-1})^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp; Now as we have derived all the formulas, we are left with only implementation of the algorithm in Python code. I'm going to attach the algorithm with the following commentaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, X, Y):\n",
    "        \n",
    "        \"\"\"We begin inputting training X and Y\"\"\"\n",
    "        \n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.m = self.Y.shape[1]    # number of examples\n",
    "        self.A = {0: self.X}        # A[0] = X\n",
    "        self.Z = {}                 # All of the below empty dictionaries will keep values\n",
    "                                                                            #of parameters\n",
    "        self.db = {}  # dictionary of vectors dJ/db, for instance db[2] = derivative of  \n",
    "                                                                #cost w.r.t.second layer b \n",
    "        self.dW = {}  # dictionary of matrices dJ/db\n",
    "        self.W, self.b = {}, {}   # dictionary of weights and biases\n",
    "    \n",
    "    def relu(self, Z):           # Activation function \n",
    "        return np.array(np.maximum(Z, 0))\n",
    "    \n",
    "    def derivrelu(self, Z):      #Derivative of activation function\n",
    "        return np.array((Z > 0).astype(int))\n",
    "    \n",
    "    def sigmoid(self, Z):        #Activation function\n",
    "        return np.array((1 + np.exp(-Z))**-1)\n",
    "    \n",
    "    def derivsigmoid(self, Z):   #Derivative of activation function\n",
    "        return np.array(self.sigmoid(Z) * (1 - self.sigmoid(Z)))\n",
    "    \n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "            We are not going to initialize weights\n",
    "        with special function, such as he initializer (it is not our goal)\n",
    "        or others, just simple random normal initialization.\n",
    "        \n",
    "        Output: Dictionaries of weight matrices and biases vector, for instance,\n",
    "        W[2] is a weight matrix to the second layer\n",
    "        \"\"\"\n",
    "        \n",
    "        # We initialize all the weights from first layer to the last\n",
    "        \n",
    "        for i in range(1, self.L + 1):\n",
    "            self.W[i] = np.random.randn(self.layers[i], self.layers[i - 1]) * 0.01     \n",
    "            self.b[i] = np.zeros((self.layers[i], 1))\n",
    "                \n",
    "    def forward_prop(self):\n",
    "        \"\"\"\n",
    "            From first layer to the L-1 layer we use Relu function\n",
    "        and the last layer \"uses\" sigmoid function. \n",
    "        \"\"\"\n",
    "        for i in range(1, self.L + 1):\n",
    "            if i != self.L:\n",
    "                func = self.relu    # Any function can be written, but \n",
    "                                         # let's for simplicity stop at ReLU\n",
    "            else:\n",
    "                func = self.sigmoid   # Any function can be written, but \n",
    "                                          # let's for simplicity stop at sigmoid\n",
    "            self.Z[i] = (self.W[i] @ self.A[i - 1]) + self.b[i] # Forward propagation\n",
    "            self.A[i] = func(self.Z[i])\n",
    "\n",
    "    def cost(self, A_L):\n",
    "        \"\"\"\n",
    "            We are going to use cross entropy loss.\n",
    "            A_L means output from the Network\n",
    "        \"\"\"\n",
    "        cost = (1 / self.m) * ((-(self.Y @ np.log(A_L.T)) - \n",
    "                               ((1 - self.Y) @ np.log(1 - A_L.T)))) \n",
    "        return float(cost)\n",
    "            \n",
    "    def back_prop(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        - dJ_dA in our code is derivative of cost J w.r.t. A[L]\n",
    "        - db[l] is is derivative of cost J w.r.t. b[l]\n",
    "        - dW[l] is is derivative of cost J w.r.t. W[l]\n",
    "        \"\"\"\n",
    "        \n",
    "        for l in reversed(range(1, self.L + 1)):     # Going from L layer to the first\n",
    "            if l == self.L:                           # We begin from the last layer L\n",
    "                dJ_dA = -(self.Y / self.A[self.L]) + ((1 - self.Y) / (1 - self.A[self.L]))\n",
    "                delta_L = dJ_dA * self.derivsigmoid(self.Z[self.L])\n",
    "                self.Delta = {self.L: delta_L}  # Just for convenience make it in dictionary\n",
    "            else:\n",
    "                # Computing from L-1 layer to the first\n",
    "                delta_l = self.derivrelu(self.Z[l]) * (self.W[l + 1].T @ self.Delta[l + 1])\n",
    "                self.Delta[l] = delta_l\n",
    "            \n",
    "            self.db[l] = (1 / self.m) * np.sum(self.Delta[l], axis=1, keepdims=True) # dJ/db\n",
    "            self.dW[l] = (1 / self.m) * (self.Delta[l] @ self.A[l-1].T)              # dJ/dW\n",
    "                \n",
    "                \n",
    "    def update_params(self):\n",
    "        \n",
    "        \"\"\"Function for updating weights\"\"\"\n",
    "        \n",
    "        for i in range(1, self.L + 1):\n",
    "            self.W[i] = self.W[i] - (self.alpha * self.dW[i])   # alpha is the learning rate\n",
    "            self.b[i] = self.b[i] - (self.alpha * self.db[i])\n",
    "                             \n",
    "        # Below we update dictionaries to empty, as we have already updated the coefficients        \n",
    "        self.A = {0: self.X}\n",
    "        self.Z = {}\n",
    "        self.db = {}\n",
    "        self.dW = {}\n",
    "        \n",
    "    def num_J(self, eps, l):\n",
    "        \n",
    "        \"\"\"\n",
    "            Input:\n",
    "            - eps - epsilon\n",
    "            - l - the current layer, to which weight matrix is \"connected\"\n",
    "            Supplementary function for numerically computing derivatives to get convinced\n",
    "        that backpropagation algorithm is implemented correctly.\n",
    "            The function is just forward propagation, from the layer,\n",
    "        where the coefficient is \"located\".\n",
    "            We use this function to compute the output of cost function with coeficeint to\n",
    "        which epsilon is added. \n",
    "        \"\"\"\n",
    "        \n",
    "        check_W = self.check_W  # check_W is copy of W\n",
    "        check_W[l] = eps\n",
    "        check_A = self.check_A\n",
    "        check_b = self.check_b\n",
    "        check_Z = self.check_Z \n",
    "                \n",
    "        for i in range(l, self.L + 1):\n",
    "            if i == self.L:\n",
    "                func = self.sigmoid\n",
    "            else:\n",
    "                func = self.relu\n",
    "            check_Z[i] = check_W[i] @ check_A[i - 1] + check_b[i] # Forward propagation \n",
    "            check_A[i] = func(check_Z[i])\n",
    "                \n",
    "        res = self.cost(check_A[i])\n",
    "        return res\n",
    "\n",
    "    def check_grad(self, l):\n",
    "        \"\"\"\n",
    "            Function check_grad is made for numerical approximation of derivatives.\n",
    "        It is necessary to be sure in the correctness of backprop implementation.\n",
    "\n",
    "        \"\"\"\n",
    "        epsilon = 1e-7    \n",
    "        row_count = self.check_W[l].shape[0]\n",
    "        col_count = self.check_W[l].shape[1] \n",
    "        gradient = np.zeros((row_count, col_count))   # Will fullfill this matrix\n",
    "        W = self.check_W[l].copy()\n",
    "        for i in range(row_count):   \n",
    "            for j in range(col_count):      # Step through each element of a matrix\n",
    "                eps = np.zeros((row_count, col_count)) \n",
    "                eps[i, j] = epsilon                   # Add epsilon to a weight \n",
    "                # Approximating derivative of one parameter\n",
    "                gradient[i, j] = (self.num_J(W + eps, l) - \n",
    "                                  self.num_J(W - eps, l)) /  (2 * epsilon)\n",
    "                \n",
    "        return gradient\n",
    "            \n",
    "    def NN(self, layers, num_iter, alpha = 0.01, check_layer = None):\n",
    "           \n",
    "        self.layers = [self.X.shape[0]] + layers  # Add X to hidden layers\n",
    "        self.L = len(self.layers) - 1             # number of layers without X\n",
    "        \n",
    "        self.alpha = alpha    # Learning rate\n",
    "        \n",
    "        self.num_iter = num_iter         # Number of iterations\n",
    "        self.initialize()               \n",
    "        for i in range(self.num_iter):   # Looping over number of epochs \n",
    "            self.forward_prop()\n",
    "            c = self.cost(self.A[self.L])\n",
    "                \n",
    "            if i % 500 == 0:\n",
    "                print('cost after iteration {}: {}'.format(i, c))\n",
    "\n",
    "            self.back_prop()\n",
    "                \n",
    "            if i == num_iter - 1:   # Check the \"numerical\" gradient from the last epoch\n",
    "                if check_layer:\n",
    "                    self.check_W = self.W.copy()\n",
    "                    self.check_A = self.A.copy() \n",
    "                    self.check_b = self.b.copy()\n",
    "                    self.check_Z = self.Z.copy()\n",
    "                    x = self.dW[check_layer]     # Check_layer is the layer, for which\n",
    "                                                 # we numerically compute the derivatives\n",
    "                                                 # x is our \"backprop\" derivative\n",
    "                            \n",
    "                    x_check = self.check_grad(check_layer)  # Approximated derivatives\n",
    "                    \n",
    "                    # dist is computing the measure of difference between two vectors - \n",
    "                    # numerically computed gradient and \"backpropagation gradient\".\n",
    "                    dist = np.linalg.norm(x - x_check) / (\n",
    "                        np.linalg.norm(x) + np.linalg.norm(x_check)\n",
    "                    )\n",
    "                    print('\\n', 'Measure of diff. between vectors is', dist)\n",
    "            self.update_params()\n",
    "        \n",
    "    def predict(self, X, Y):\n",
    "        \"\"\"\n",
    "            Prediction for Y with two classes, just for one example on which\n",
    "        we are going to test our model\n",
    "        \"\"\"\n",
    "        A = {0: X}\n",
    "        Z = {}\n",
    "        for i in range(1, self.L):\n",
    "            Z[i] = (self.W[i] @ A[i - 1]) + self.b[i]\n",
    "            A[i] = self.relu(Z[i])\n",
    "        Z[i + 1] = (self.W[i + 1] @ A[i]) + self.b[i + 1]\n",
    "        A[i + 1] = self.sigmoid(Z[i + 1])\n",
    "        Result = (A[i + 1] > 0.5).astype(int)\n",
    "        end = ((Result @ Y.T) + ((1 - Result) @ (1 - Y).T))\n",
    "        return float(end / Y.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Let's check our model on a small data set, containing 209 train pictures and 50 test pictures. We will build a model, which recognize cats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "test = h5py.File('test_catvnoncat.h5', 'r')\n",
    "train = h5py.File('train_catvnoncat.h5', 'r')\n",
    "\n",
    "train_X_orig = np.array(list(train.values())[1])\n",
    "train_y_orig = np.array(list(train.values())[2])\n",
    "\n",
    "train_X = (train_X_orig.reshape(train_X_orig.shape[0], -1).T) / 255  # Normalized training X\n",
    "train_y = train_y_orig.reshape(1, 209)                               # Training y\n",
    "\n",
    "test_X_orig = np.array(list(test.values())[1])\n",
    "test_y_orig = np.array(list(test.values())[2])\n",
    "\n",
    "test_X = (test_X_orig.reshape(test_X_orig.shape[0], -1).T) / 255     # Normalized test X\n",
    "test_y = test_y_orig.reshape(1, 50)                                  # Test y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = NeuralNetwork(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [7, 1]         # Units in layers from first to the last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after iteration 0: 0.6912523365403378\n",
      "cost after iteration 500: 0.4916869860498198\n",
      "cost after iteration 1000: 0.33203141408879544\n",
      "cost after iteration 1500: 0.12464979421409468\n",
      "cost after iteration 2000: 0.05639039782118585\n",
      "cost after iteration 2500: 0.03314290801621348\n",
      "\n",
      " Measure of diff. between vectors is 7.96850499225316e-10\n"
     ]
    }
   ],
   "source": [
    "# Let's check the gradient for the second layer \n",
    "x.NN(layers, 2501, alpha=0.0077, check_layer=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.predict(test_X, test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
